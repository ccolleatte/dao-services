{
  "topic": "cryptoeconomics",
  "time_period": "2024-2025",
  "last_updated": "2026-02-09T19:30:00Z",
  "total_papers": 5,
  "curation_status": "Phase 1 - Complete",
  "priority": "P3 - Peripheral topics",
  "papers": [
    {
      "arxiv_id": "2602.06911v1",
      "title": "TamperBench: Systematically Stress-Testing LLM Safety Under Fine-Tuning and Tampering",
      "authors": [
        "Saad Hossain",
        "Tom Tseng",
        "Punya Syon Pandey",
        "Samanvay Vajpayee",
        "Matthew Kowal",
        "Nayeema Nonta",
        "Samuel Simko",
        "Stephen Casper",
        "Zhijing Jin",
        "Kellin Pelrine",
        "Sirisha Rambhatla"
      ],
      "submitted_date": "2026-02-06",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "abstract": "As increasingly capable open-weight large language models (LLMs) are deployed, improving their tamper resistance against unsafe modifications, whether accidental or intentional, becomes critical to minimize risks. However, there is no standard approach to evaluate tamper resistance. Varied data sets, metrics, and tampering configurations make it difficult to compare safety, utility, and robustness across different models and defenses. To this end, we introduce TamperBench, the first unified framework to systematically evaluate the tamper resistance of LLMs. TamperBench (i) curates a repository of state-of-the-art weight-space fine-tuning attacks and latent-space representation attacks; (ii) enables realistic adversarial evaluation through systematic hyperparameter sweeps per attack-model pair; and (iii) provides both safety and utility evaluations. TamperBench requires minimal additional code to specify any fine-tuning configuration, alignment-stage defense method, and metric suite while ensuring end-to-end reproducibility. We use TamperBench to evaluate 21 open-weight LLMs, including defense-augmented variants, across nine tampering threats using standardized safety and capability metrics with hyperparameter sweeps per model-attack pair. This yields novel insights, including effects of post-training on tamper resistance, that jailbreak-tuning is typically the most severe attack, and that Triplet emerges as a leading alignment-stage defense. Code is available at: https://github.com/criticalml-uw/TamperBench",
      "pdf_url": "https://arxiv.org/pdf/2602.06911v1",
      "relevance_score": 7,
      "scoring_breakdown": {
        "topic_match": 1,
        "methodology_rigor": 2,
        "citation_potential": 2,
        "recency": 2
      },
      "notes": "MEDIUM - High citation potential.",
      "tags": [],
      "citation_count": 0,
      "integration_status": "pending_review",
      "pdf_stored_locally": false
    },
    {
      "arxiv_id": "2602.06751v1",
      "title": "Beyond Function-Level Analysis: Context-Aware Reasoning for Inter-Procedural Vulnerability Detection",
      "authors": [
        "Yikun Li",
        "Ting Zhang",
        "Jieke Shi",
        "Chengran Yang",
        "Junda He",
        "Xin Zhou",
        "Jinfeng Jiang",
        "Huihui Huang",
        "Wen Bin Leow",
        "Yide Yin",
        "Eng Lieh Ouh",
        "Lwin Khin Shar",
        "David Lo"
      ],
      "submitted_date": "2026-02-06",
      "categories": [
        "cs.CR",
        "cs.SE"
      ],
      "abstract": "Recent progress in ML and LLMs has improved vulnerability detection, and recent datasets have reduced label noise and unrelated code changes. However, most existing approaches still operate at the function level, where models are asked to predict whether a single function is vulnerable without inter-procedural context. In practice, vulnerability presence and root cause often depend on contextual information. Naively appending such context is not a reliable solution: real-world context is long, redundant, and noisy, and we find that unstructured context frequently degrades the performance of strong fine-tuned code models.   We present CPRVul, a context-aware vulnerability detection framework that couples Context Profiling and Selection with Structured Reasoning. CPRVul constructs a code property graph, and extracts candidate context. It then uses an LLM to generate security-focused profiles and assign relevance scores, selecting only high-impact contextual elements that fit within the model's context window. In the second phase, CPRVul integrates the target function, the selected context, and auxiliary vulnerability metadata to generate reasoning traces, which are used to fine-tune LLMs for reasoning-based vulnerability detection.   We evaluate CPRVul on three high-quality vulnerability datasets: PrimeVul, TitanVul, and CleanVul. Across all datasets, CPRVul consistently outperforms function-only baselines, achieving accuracies ranging from 64.94% to 73.76%, compared to 56.65% to 63.68% for UniXcoder. Specifically, on the challenging PrimeVul benchmark, CPRVul achieves 67.78% accuracy, outperforming prior state-of-the-art approaches, improving accuracy from 55.17% to 67.78% (22.9% improvement). Our ablations further show that neither raw context nor processed context alone benefits strong code models; gains emerge only when processed context is paired with structured reasoning.",
      "pdf_url": "https://arxiv.org/pdf/2602.06751v1",
      "relevance_score": 7,
      "scoring_breakdown": {
        "topic_match": 1,
        "methodology_rigor": 2,
        "citation_potential": 2,
        "recency": 2
      },
      "notes": "MEDIUM - High citation potential.",
      "tags": [],
      "citation_count": 0,
      "integration_status": "pending_review",
      "pdf_stored_locally": false
    },
    {
      "arxiv_id": "2602.06718v1",
      "title": "GhostCite: A Large-Scale Analysis of Citation Validity in the Age of Large Language Models",
      "authors": [
        "Zuyao Xu",
        "Yuqi Qiu",
        "Lu Sun",
        "FaSheng Miao",
        "Fubin Wu",
        "Xinyi Wang",
        "Xiang Li",
        "Haozhe Lu",
        "ZhengZe Zhang",
        "Yuxin Hu",
        "Jialu Li",
        "Jin Luo",
        "Feng Zhang",
        "Rui Luo",
        "Xinran Liu",
        "Yingxian Li",
        "Jiaji Liu"
      ],
      "submitted_date": "2026-02-06",
      "categories": [
        "cs.CR",
        "cs.AI"
      ],
      "abstract": "Citations provide the basis for trusting scientific claims; when they are invalid or fabricated, this trust collapses. With the advent of Large Language Models (LLMs), this risk has intensified: LLMs are increasingly used for academic writing, yet their tendency to fabricate citations (``ghost citations'') poses a systemic threat to citation validity.   To quantify this threat and inform mitigation, we develop CiteVerifier, an open-source framework for large-scale citation verification, and conduct the first comprehensive study of citation validity in the LLM era through three experiments built on it. We benchmark 13 state-of-the-art LLMs on citation generation across 40 research domains, finding that all models hallucinate citations at rates from 14.23\\% to 94.93\\%, with significant variation across research domains. Moreover, we analyze 2.2 million citations from 56,381 papers published at top-tier AI/ML and Security venues (2020--2025), confirming that 1.07\\% of papers contain invalid or fabricated citations (604 papers), with an 80.9\\% increase in 2025 alone. Furthermore, we survey 97 researchers and analyze 94 valid responses after removing 3 conflicting samples, revealing a critical ``verification gap'': 41.5\\% of researchers copy-paste BibTeX without checking and 44.4\\% choose no-action responses when encountering suspicious references; meanwhile, 76.7\\% of reviewers do not thoroughly check references and 80.0\\% never suspect fake citations. Our findings reveal an accelerating crisis where unreliable AI tools, combined with inadequate human verification by researchers and insufficient peer review scrutiny, enable fabricated citations to contaminate the scientific record. We propose interventions for researchers, venues, and tool developers to protect citation integrity.",
      "pdf_url": "https://arxiv.org/pdf/2602.06718v1",
      "relevance_score": 7,
      "scoring_breakdown": {
        "topic_match": 1,
        "methodology_rigor": 2,
        "citation_potential": 2,
        "recency": 2
      },
      "notes": "MEDIUM - High citation potential.",
      "tags": [],
      "citation_count": 0,
      "integration_status": "pending_review",
      "pdf_stored_locally": false
    },
    {
      "arxiv_id": "2602.06700v1",
      "title": "Taipan: A Query-free Transfer-based Multiple Sensitive Attribute Inference Attack Solely from Publicly Released Graphs",
      "authors": [
        "Ying Song",
        "Balaji Palanisamy"
      ],
      "submitted_date": "2026-02-06",
      "categories": [
        "cs.CR",
        "cs.LG"
      ],
      "abstract": "Graph-structured data underpin a wide spectrum of modern applications. However, complex graph topologies and homophilic patterns can facilitate attribute inference attacks (AIAs) by enabling sensitive information leakage to propagate across local neighborhoods. Existing AIAs predominantly assume that adversaries can probe sensitive attributes through repeated model queries. Such assumptions are often impractical in real-world settings due to stringent data protection regulations, prohibitive query budgets, and heightened detection risks, especially when inferring multiple sensitive attributes. More critically, this model-centric perspective obscures a pervasive blind spot: \\textbf{intrinsic multiple sensitive information leakage arising solely from publicly released graphs.} To exploit this unexplored vulnerability, we introduce a new attack paradigm and propose \\textbf{Taipan, the first query-free transfer-based attack framework for multiple sensitive attribute inference attacks on graphs (G-MSAIAs).} Taipan integrates \\emph{Hierarchical Attack Knowledge Routing} to capture intricate inter-attribute correlations, and \\emph{Prompt-guided Attack Prototype Refinement} to mitigate negative transfer and performance degradation. We further present a systematic evaluation framework tailored to G-MSAIAs. Extensive experiments on diverse real-world graph datasets demonstrate that Taipan consistently achieves strong attack performance across same-distribution settings and heterogeneous similar- and out-of-distribution settings with mismatched feature dimensionalities, and remains effective even under rigorous differential privacy guarantees. Our findings underscore the urgent need for more robust multi-attribute privacy-preserving graph publishing methods and data-sharing practices.",
      "pdf_url": "https://arxiv.org/pdf/2602.06700v1",
      "relevance_score": 7,
      "scoring_breakdown": {
        "topic_match": 1,
        "methodology_rigor": 2,
        "citation_potential": 2,
        "recency": 2
      },
      "notes": "MEDIUM - High citation potential.",
      "tags": [],
      "citation_count": 0,
      "integration_status": "pending_review",
      "pdf_stored_locally": false
    },
    {
      "arxiv_id": "2602.06687v1",
      "title": "Evaluating and Enhancing the Vulnerability Reasoning Capabilities of Large Language Models",
      "authors": [
        "Li Lu",
        "Yanjie Zhao",
        "Hongzhou Rao",
        "Kechi Zhang",
        "Haoyu Wang"
      ],
      "submitted_date": "2026-02-06",
      "categories": [
        "cs.CR"
      ],
      "abstract": "Large Language Models (LLMs) have demonstrated remarkable proficiency in vulnerability detection. However, a critical reliability gap persists: models frequently yield correct detection verdicts based on hallucinated logic or superficial patterns that deviate from the actual root cause. This misalignment remains largely obscured because contemporary benchmarks predominantly prioritize coarse-grained classification metrics, lacking the granular ground truth required to evaluate the underlying reasoning process. To bridge this gap, we first construct a benchmark consisting of two datasets: (1) real-world vulnerabilities with expert-curated causal reasoning as ground truth, and (2) semantically equivalent code perturbations for assessing reasoning robustness. Our large-scale empirical study reveals that even state-of-the-art models struggle to maintain logical consistency during semantic code comprehension, exhibiting 12 systematic failure patterns. Addressing these limitations, we propose DAGVul, a novel framework that models vulnerability reasoning as a Directed Acyclic Graph (DAG) generation task. Unlike linear chain-of-thought (CoT), our approach explicitly maps causal dependencies to enforce structural consistency. By further introducing Reinforcement Learning with Verifiable Rewards (RLVR), we align model reasoning trace with program-intrinsic logic. Experimental results demonstrate that our framework improves the reasoning F1-score by an average of 18.9% over all the baselines. Remarkably, our 8B-parameter implementation not only outperforms existing models of comparable scale but also surpasses specialized large-scale reasoning models, including Qwen3-30B-Reasoning and GPT-OSS-20B-High. It is even competitive with state-of-the-art models like Claude-Sonnet-4.5 (75.47% vs. 76.11%), establishing new efficiency in vulnerability reasoning across model scales.",
      "pdf_url": "https://arxiv.org/pdf/2602.06687v1",
      "relevance_score": 7,
      "scoring_breakdown": {
        "topic_match": 1,
        "methodology_rigor": 2,
        "citation_potential": 2,
        "recency": 2
      },
      "notes": "MEDIUM - High citation potential.",
      "tags": [],
      "citation_count": 0,
      "integration_status": "pending_review",
      "pdf_stored_locally": false
    }
  ]
}